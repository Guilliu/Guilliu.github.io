{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pyken - Scorecard development with Python Pyken is a Python package with the aim of providing the necessary tools for: Grouping variables (both numerical and categorical) in an automatic and interactive way. Development of customizable scorecards adaptable to the requirements of each user. Source code Check out the GitHub repository. Installation You can install Pyken using pip pip install pyken","title":"Intro"},{"location":"#pyken-scorecard-development-with-python","text":"Pyken is a Python package with the aim of providing the necessary tools for: Grouping variables (both numerical and categorical) in an automatic and interactive way. Development of customizable scorecards adaptable to the requirements of each user.","title":"Pyken - Scorecard development with Python"},{"location":"#source-code","text":"Check out the GitHub repository.","title":"Source code"},{"location":"#installation","text":"You can install Pyken using pip pip install pyken","title":"Installation"},{"location":"autoscorecard/","text":"Tengo pendiente hacer esto... Habr\u00eda que poner como funciona cada uno de los tropecientos mil par\u00e1metros de la clase..","title":"Autoscorecard"},{"location":"examples/","text":"Examples (Esto habr\u00eda que traducirlo al ingl\u00e9s digo yo...) Informaci\u00f3n Los ejemplos pueden encontrarse en formato .ipynb y .html en el siguiente enlace . Tambi\u00e9n se encuentran los datasets necesarios y alg\u00fan otro archivo .xlsx o .csv que se genera. Recomiendo descargarlos para ir viendo como se ejecuta el c\u00f3digo. Pongo el primero de ellos en esta documentaci\u00f3n al ser un ejemplo ilustrativo con lo m\u00e1s b\u00e1sico. Ejemplo sencillo En este notebook se muestra la clase autoscorecard aplicada en un dataset de juguete. En este dataset la variable objetivo est\u00e1 muy correlada con el resto de variables Importamos los m\u00f3dulos import numpy as np, pandas as pd, pyken as pyk Cargamos el dataset separando las variables predictoras de la variable objetivo from sklearn.datasets import load_breast_cancer as lbc X, y = pd.DataFrame(lbc().data, columns=lbc().feature_names), lbc().target Echamos un vistazo al dataset X[X.columns[:10]].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension 0 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 2 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 3 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 4 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 No es buena pr\u00e1ctica tener espacios en los nombres de las columnas, mejor ponemos guiones bajos X.columns = [i.replace(' ', '_') for i in X.columns] Aplicamos la clase autoscorecard para sacar el modelo autom\u00e1tico modelo1 = pyk.autoscorecard().fit(X, y) Particionado 70-30 estratificado en el target terminado ------------------------------------------------------------------------------------------------------------------------------------------------------ Autogrouping terminado. M\u00e1ximo n\u00famero de buckets = 5. M\u00ednimo porcentaje por bucket = 0.05 ------------------------------------------------------------------------------------------------------------------------------------------------------ Step 01 | Time - 0:00:00.359722 | p-value = 4.93e-32 | Gini train = 83.97% | Gini test = 87.30% ---> Feature selected: mean_concavity Step 02 | Time - 0:00:00.392056 | p-value = 1.38e-14 | Gini train = 96.82% | Gini test = 97.24% ---> Feature selected: worst_perimeter Step 03 | Time - 0:00:00.405165 | p-value = 4.31e-06 | Gini train = 98.34% | Gini test = 98.07% ---> Feature selected: worst_texture Step 04 | Time - 0:00:00.577206 | p-value = 5.11e-04 | Gini train = 98.92% | Gini test = 97.06% ---> Feature selected: worst_smoothness Step 05 | Time - 0:00:00.706235 | p-value = 1.62e-03 | Gini train = 99.34% | Gini test = 98.51% ---> Feature selected: radius_error Step 05 | Time - 0:00:00.000000 | p-value = 1.54e-02 | Gini train = 99.25% | Gini test = 98.22% ---> Feature deleted : mean_concavity Step 06 | Time - 0:00:00.691006 | p-value = 2.28e-03 | Gini train = 99.60% | Gini test = 98.77% ---> Feature selected: worst_concavity ------------------------------------------------------------------------------------------------------------------------------------------------------ Ya ninguna variable tiene un p-valor < 0.01, detenemos el proceso ------------------------------------------------------------------------------------------------------------------------------------------------------ Selecci\u00f3n terminada: ['worst_perimeter', 'worst_texture', 'worst_smoothness', 'radius_error', 'worst_concavity'] ------------------------------------------------------------------------------------------------------------------------------------------------------ El modelo tiene un 95.55% de KS y un 99.60% de Gini en la muestra de desarrollo ------------------------------------------------------------------------------------------------------------------------------------------------------ El modelo tiene un 95.63% de KS y un 98.77% de Gini en la muestra de validaci\u00f3n ------------------------------------------------------------------------------------------------------------------------------------------------------ Pintamos la scorecard con colorines pyk.pretty_scorecard(modelo1) #T_e2d9d_row0_col0, #T_e2d9d_row0_col1, #T_e2d9d_row0_col2, #T_e2d9d_row0_col3, #T_e2d9d_row0_col4, #T_e2d9d_row0_col5, #T_e2d9d_row0_col6, #T_e2d9d_row0_col7, #T_e2d9d_row0_col8, #T_e2d9d_row0_col9, #T_e2d9d_row0_col10, #T_e2d9d_row1_col0, #T_e2d9d_row1_col1, #T_e2d9d_row1_col2, #T_e2d9d_row1_col3, #T_e2d9d_row1_col4, #T_e2d9d_row1_col5, #T_e2d9d_row1_col6, #T_e2d9d_row1_col7, #T_e2d9d_row1_col8, #T_e2d9d_row1_col9, #T_e2d9d_row1_col10, #T_e2d9d_row2_col0, #T_e2d9d_row2_col1, #T_e2d9d_row2_col2, #T_e2d9d_row2_col3, #T_e2d9d_row2_col4, #T_e2d9d_row2_col5, #T_e2d9d_row2_col6, #T_e2d9d_row2_col7, #T_e2d9d_row2_col8, #T_e2d9d_row2_col9, #T_e2d9d_row2_col10, #T_e2d9d_row3_col0, #T_e2d9d_row3_col1, #T_e2d9d_row3_col2, #T_e2d9d_row3_col3, #T_e2d9d_row3_col4, #T_e2d9d_row3_col5, #T_e2d9d_row3_col6, #T_e2d9d_row3_col7, #T_e2d9d_row3_col8, #T_e2d9d_row3_col9, #T_e2d9d_row3_col10, #T_e2d9d_row9_col0, #T_e2d9d_row9_col1, #T_e2d9d_row9_col2, #T_e2d9d_row9_col3, #T_e2d9d_row9_col4, #T_e2d9d_row9_col5, #T_e2d9d_row9_col6, #T_e2d9d_row9_col7, #T_e2d9d_row9_col8, #T_e2d9d_row9_col9, #T_e2d9d_row9_col10, #T_e2d9d_row10_col0, #T_e2d9d_row10_col1, #T_e2d9d_row10_col2, #T_e2d9d_row10_col3, #T_e2d9d_row10_col4, #T_e2d9d_row10_col5, #T_e2d9d_row10_col6, #T_e2d9d_row10_col7, #T_e2d9d_row10_col8, #T_e2d9d_row10_col9, #T_e2d9d_row10_col10, #T_e2d9d_row11_col0, #T_e2d9d_row11_col1, #T_e2d9d_row11_col2, #T_e2d9d_row11_col3, #T_e2d9d_row11_col4, #T_e2d9d_row11_col5, #T_e2d9d_row11_col6, #T_e2d9d_row11_col7, #T_e2d9d_row11_col8, #T_e2d9d_row11_col9, #T_e2d9d_row11_col10, #T_e2d9d_row12_col0, #T_e2d9d_row12_col1, #T_e2d9d_row12_col2, #T_e2d9d_row12_col3, #T_e2d9d_row12_col4, #T_e2d9d_row12_col5, #T_e2d9d_row12_col6, #T_e2d9d_row12_col7, #T_e2d9d_row12_col8, #T_e2d9d_row12_col9, #T_e2d9d_row12_col10, #T_e2d9d_row13_col0, #T_e2d9d_row13_col1, #T_e2d9d_row13_col2, #T_e2d9d_row13_col3, #T_e2d9d_row13_col4, #T_e2d9d_row13_col5, #T_e2d9d_row13_col6, #T_e2d9d_row13_col7, #T_e2d9d_row13_col8, #T_e2d9d_row13_col9, #T_e2d9d_row13_col10, #T_e2d9d_row19_col0, #T_e2d9d_row19_col1, #T_e2d9d_row19_col2, #T_e2d9d_row19_col3, #T_e2d9d_row19_col4, #T_e2d9d_row19_col5, #T_e2d9d_row19_col6, #T_e2d9d_row19_col7, #T_e2d9d_row19_col8, #T_e2d9d_row19_col9, #T_e2d9d_row19_col10, #T_e2d9d_row20_col0, #T_e2d9d_row20_col1, #T_e2d9d_row20_col2, #T_e2d9d_row20_col3, #T_e2d9d_row20_col4, #T_e2d9d_row20_col5, #T_e2d9d_row20_col6, #T_e2d9d_row20_col7, #T_e2d9d_row20_col8, #T_e2d9d_row20_col9, #T_e2d9d_row20_col10, #T_e2d9d_row21_col0, #T_e2d9d_row21_col1, #T_e2d9d_row21_col2, #T_e2d9d_row21_col3, #T_e2d9d_row21_col4, #T_e2d9d_row21_col5, #T_e2d9d_row21_col6, #T_e2d9d_row21_col7, #T_e2d9d_row21_col8, #T_e2d9d_row21_col9, #T_e2d9d_row21_col10, #T_e2d9d_row22_col0, #T_e2d9d_row22_col1, #T_e2d9d_row22_col2, #T_e2d9d_row22_col3, #T_e2d9d_row22_col4, #T_e2d9d_row22_col5, #T_e2d9d_row22_col6, #T_e2d9d_row22_col7, #T_e2d9d_row22_col8, #T_e2d9d_row22_col9, #T_e2d9d_row22_col10, #T_e2d9d_row23_col0, #T_e2d9d_row23_col1, #T_e2d9d_row23_col2, #T_e2d9d_row23_col3, #T_e2d9d_row23_col4, #T_e2d9d_row23_col5, #T_e2d9d_row23_col6, #T_e2d9d_row23_col7, #T_e2d9d_row23_col8, #T_e2d9d_row23_col9, #T_e2d9d_row23_col10 { background-color: #CCECFF; } #T_e2d9d_row4_col0, #T_e2d9d_row4_col1, #T_e2d9d_row4_col2, #T_e2d9d_row4_col3, #T_e2d9d_row4_col4, #T_e2d9d_row4_col5, #T_e2d9d_row4_col6, #T_e2d9d_row4_col7, #T_e2d9d_row4_col8, #T_e2d9d_row4_col9, #T_e2d9d_row4_col10, #T_e2d9d_row5_col0, #T_e2d9d_row5_col1, #T_e2d9d_row5_col2, #T_e2d9d_row5_col3, #T_e2d9d_row5_col4, #T_e2d9d_row5_col5, #T_e2d9d_row5_col6, #T_e2d9d_row5_col7, #T_e2d9d_row5_col8, #T_e2d9d_row5_col9, #T_e2d9d_row5_col10, #T_e2d9d_row6_col0, #T_e2d9d_row6_col1, #T_e2d9d_row6_col2, #T_e2d9d_row6_col3, #T_e2d9d_row6_col4, #T_e2d9d_row6_col5, #T_e2d9d_row6_col6, #T_e2d9d_row6_col7, #T_e2d9d_row6_col8, #T_e2d9d_row6_col9, #T_e2d9d_row6_col10, #T_e2d9d_row7_col0, #T_e2d9d_row7_col1, #T_e2d9d_row7_col2, #T_e2d9d_row7_col3, #T_e2d9d_row7_col4, #T_e2d9d_row7_col5, #T_e2d9d_row7_col6, #T_e2d9d_row7_col7, #T_e2d9d_row7_col8, #T_e2d9d_row7_col9, #T_e2d9d_row7_col10, #T_e2d9d_row8_col0, #T_e2d9d_row8_col1, #T_e2d9d_row8_col2, #T_e2d9d_row8_col3, #T_e2d9d_row8_col4, #T_e2d9d_row8_col5, #T_e2d9d_row8_col6, #T_e2d9d_row8_col7, #T_e2d9d_row8_col8, #T_e2d9d_row8_col9, #T_e2d9d_row8_col10, #T_e2d9d_row14_col0, #T_e2d9d_row14_col1, #T_e2d9d_row14_col2, #T_e2d9d_row14_col3, #T_e2d9d_row14_col4, #T_e2d9d_row14_col5, #T_e2d9d_row14_col6, #T_e2d9d_row14_col7, #T_e2d9d_row14_col8, #T_e2d9d_row14_col9, #T_e2d9d_row14_col10, #T_e2d9d_row15_col0, #T_e2d9d_row15_col1, #T_e2d9d_row15_col2, #T_e2d9d_row15_col3, #T_e2d9d_row15_col4, #T_e2d9d_row15_col5, #T_e2d9d_row15_col6, #T_e2d9d_row15_col7, #T_e2d9d_row15_col8, #T_e2d9d_row15_col9, #T_e2d9d_row15_col10, #T_e2d9d_row16_col0, #T_e2d9d_row16_col1, #T_e2d9d_row16_col2, #T_e2d9d_row16_col3, #T_e2d9d_row16_col4, #T_e2d9d_row16_col5, #T_e2d9d_row16_col6, #T_e2d9d_row16_col7, #T_e2d9d_row16_col8, #T_e2d9d_row16_col9, #T_e2d9d_row16_col10, #T_e2d9d_row17_col0, #T_e2d9d_row17_col1, #T_e2d9d_row17_col2, #T_e2d9d_row17_col3, #T_e2d9d_row17_col4, #T_e2d9d_row17_col5, #T_e2d9d_row17_col6, #T_e2d9d_row17_col7, #T_e2d9d_row17_col8, #T_e2d9d_row17_col9, #T_e2d9d_row17_col10, #T_e2d9d_row18_col0, #T_e2d9d_row18_col1, #T_e2d9d_row18_col2, #T_e2d9d_row18_col3, #T_e2d9d_row18_col4, #T_e2d9d_row18_col5, #T_e2d9d_row18_col6, #T_e2d9d_row18_col7, #T_e2d9d_row18_col8, #T_e2d9d_row18_col9, #T_e2d9d_row18_col10 { background-color: #FFFFFF; } Variable Group Count Percent Goods Bads Bad rate WoE IV Raw score Aligned score 0 worst_perimeter (-inf, 91.69) 167 0.419598 2 165 0.988024 -3.888550 2.513895 5.210485 -48 1 worst_perimeter [91.69, 102.05) 58 0.145729 5 53 0.913793 -1.836605 0.327313 2.460970 32 2 worst_perimeter [102.05, 114.65) 52 0.130653 23 29 0.557692 0.292447 0.011524 -0.391866 114 3 worst_perimeter [114.65, inf) 121 0.304020 118 3 0.024793 4.196321 3.295360 -5.622885 265 4 worst_texture (-inf, 23.35) 159 0.399497 19 140 0.880503 -1.472955 0.635759 2.334961 35 5 worst_texture [23.35, 28.24) 112 0.281407 54 58 0.517857 0.452790 0.060160 -0.717772 123 6 worst_texture [28.24, 29.23) 20 0.050251 5 15 0.750000 -0.574364 0.015058 0.910494 76 7 worst_texture [29.23, 31.17) 31 0.077889 26 5 0.161290 2.172907 0.338269 -3.444541 202 8 worst_texture [31.17, inf) 76 0.190955 44 32 0.421053 0.842702 0.142667 -1.335871 141 9 worst_smoothness (-inf, 0.10) 34 0.085427 1 33 0.970588 -2.972259 0.372255 6.443867 -83 10 worst_smoothness [0.10, 0.13) 130 0.326633 33 97 0.746154 -0.553955 0.091418 1.200976 68 11 worst_smoothness [0.13, 0.14) 63 0.158291 11 52 0.825397 -1.029100 0.137566 2.231092 38 12 worst_smoothness [0.14, 0.16) 114 0.286432 60 54 0.473684 0.629609 0.119251 -1.364995 142 13 worst_smoothness [0.16, inf) 57 0.143216 43 14 0.245614 1.646391 0.386146 -3.569382 206 14 radius_error (-inf, 0.24) 111 0.278894 6 105 0.945946 -2.337952 0.887158 3.846413 -8 15 radius_error [0.24, 0.41) 152 0.381910 35 117 0.769737 -0.682577 0.158026 1.122980 70 16 radius_error [0.41, 0.48) 32 0.080402 23 9 0.281250 1.462518 0.174633 -2.406144 172 17 radius_error [0.48, 0.56) 26 0.065327 12 14 0.538462 0.370098 0.009282 -0.608887 120 18 radius_error [0.56, inf) 77 0.193467 72 5 0.064935 3.191477 1.488781 -5.250637 254 19 worst_concavity (-inf, 0.21) 182 0.457286 4 178 0.978022 -3.271241 2.240711 2.492782 31 20 worst_concavity [0.21, 0.26) 39 0.097990 12 27 0.692308 -0.286682 0.007717 0.218460 96 21 worst_concavity [0.26, 0.29) 20 0.050251 14 6 0.300000 1.371547 0.096824 -1.045159 133 22 worst_concavity [0.29, 0.38) 53 0.133166 26 27 0.509434 0.486508 0.032925 -0.370734 113 23 worst_concavity [0.38, inf) 104 0.261307 92 12 0.115385 2.561131 1.469120 -1.951657 159 El modelo tiene una hoja de c\u00e1lculo con la scorecard pintanda con los formatos adecuados. modelo1.save_excel('scorecard_ejemplo01.xlsx')","title":"Examples"},{"location":"examples/#examples","text":"(Esto habr\u00eda que traducirlo al ingl\u00e9s digo yo...)","title":"Examples"},{"location":"examples/#informacion","text":"Los ejemplos pueden encontrarse en formato .ipynb y .html en el siguiente enlace . Tambi\u00e9n se encuentran los datasets necesarios y alg\u00fan otro archivo .xlsx o .csv que se genera. Recomiendo descargarlos para ir viendo como se ejecuta el c\u00f3digo. Pongo el primero de ellos en esta documentaci\u00f3n al ser un ejemplo ilustrativo con lo m\u00e1s b\u00e1sico.","title":"Informaci\u00f3n"},{"location":"examples/#ejemplo-sencillo","text":"En este notebook se muestra la clase autoscorecard aplicada en un dataset de juguete. En este dataset la variable objetivo est\u00e1 muy correlada con el resto de variables Importamos los m\u00f3dulos import numpy as np, pandas as pd, pyken as pyk Cargamos el dataset separando las variables predictoras de la variable objetivo from sklearn.datasets import load_breast_cancer as lbc X, y = pd.DataFrame(lbc().data, columns=lbc().feature_names), lbc().target Echamos un vistazo al dataset X[X.columns[:10]].head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension 0 17.99 10.38 122.80 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1 20.57 17.77 132.90 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 2 19.69 21.25 130.00 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 3 11.42 20.38 77.58 386.1 0.14250 0.28390 0.2414 0.10520 0.2597 0.09744 4 20.29 14.34 135.10 1297.0 0.10030 0.13280 0.1980 0.10430 0.1809 0.05883 No es buena pr\u00e1ctica tener espacios en los nombres de las columnas, mejor ponemos guiones bajos X.columns = [i.replace(' ', '_') for i in X.columns] Aplicamos la clase autoscorecard para sacar el modelo autom\u00e1tico modelo1 = pyk.autoscorecard().fit(X, y) Particionado 70-30 estratificado en el target terminado ------------------------------------------------------------------------------------------------------------------------------------------------------ Autogrouping terminado. M\u00e1ximo n\u00famero de buckets = 5. M\u00ednimo porcentaje por bucket = 0.05 ------------------------------------------------------------------------------------------------------------------------------------------------------ Step 01 | Time - 0:00:00.359722 | p-value = 4.93e-32 | Gini train = 83.97% | Gini test = 87.30% ---> Feature selected: mean_concavity Step 02 | Time - 0:00:00.392056 | p-value = 1.38e-14 | Gini train = 96.82% | Gini test = 97.24% ---> Feature selected: worst_perimeter Step 03 | Time - 0:00:00.405165 | p-value = 4.31e-06 | Gini train = 98.34% | Gini test = 98.07% ---> Feature selected: worst_texture Step 04 | Time - 0:00:00.577206 | p-value = 5.11e-04 | Gini train = 98.92% | Gini test = 97.06% ---> Feature selected: worst_smoothness Step 05 | Time - 0:00:00.706235 | p-value = 1.62e-03 | Gini train = 99.34% | Gini test = 98.51% ---> Feature selected: radius_error Step 05 | Time - 0:00:00.000000 | p-value = 1.54e-02 | Gini train = 99.25% | Gini test = 98.22% ---> Feature deleted : mean_concavity Step 06 | Time - 0:00:00.691006 | p-value = 2.28e-03 | Gini train = 99.60% | Gini test = 98.77% ---> Feature selected: worst_concavity ------------------------------------------------------------------------------------------------------------------------------------------------------ Ya ninguna variable tiene un p-valor < 0.01, detenemos el proceso ------------------------------------------------------------------------------------------------------------------------------------------------------ Selecci\u00f3n terminada: ['worst_perimeter', 'worst_texture', 'worst_smoothness', 'radius_error', 'worst_concavity'] ------------------------------------------------------------------------------------------------------------------------------------------------------ El modelo tiene un 95.55% de KS y un 99.60% de Gini en la muestra de desarrollo ------------------------------------------------------------------------------------------------------------------------------------------------------ El modelo tiene un 95.63% de KS y un 98.77% de Gini en la muestra de validaci\u00f3n ------------------------------------------------------------------------------------------------------------------------------------------------------ Pintamos la scorecard con colorines pyk.pretty_scorecard(modelo1) #T_e2d9d_row0_col0, #T_e2d9d_row0_col1, #T_e2d9d_row0_col2, #T_e2d9d_row0_col3, #T_e2d9d_row0_col4, #T_e2d9d_row0_col5, #T_e2d9d_row0_col6, #T_e2d9d_row0_col7, #T_e2d9d_row0_col8, #T_e2d9d_row0_col9, #T_e2d9d_row0_col10, #T_e2d9d_row1_col0, #T_e2d9d_row1_col1, #T_e2d9d_row1_col2, #T_e2d9d_row1_col3, #T_e2d9d_row1_col4, #T_e2d9d_row1_col5, #T_e2d9d_row1_col6, #T_e2d9d_row1_col7, #T_e2d9d_row1_col8, #T_e2d9d_row1_col9, #T_e2d9d_row1_col10, #T_e2d9d_row2_col0, #T_e2d9d_row2_col1, #T_e2d9d_row2_col2, #T_e2d9d_row2_col3, #T_e2d9d_row2_col4, #T_e2d9d_row2_col5, #T_e2d9d_row2_col6, #T_e2d9d_row2_col7, #T_e2d9d_row2_col8, #T_e2d9d_row2_col9, #T_e2d9d_row2_col10, #T_e2d9d_row3_col0, #T_e2d9d_row3_col1, #T_e2d9d_row3_col2, #T_e2d9d_row3_col3, #T_e2d9d_row3_col4, #T_e2d9d_row3_col5, #T_e2d9d_row3_col6, #T_e2d9d_row3_col7, #T_e2d9d_row3_col8, #T_e2d9d_row3_col9, #T_e2d9d_row3_col10, #T_e2d9d_row9_col0, #T_e2d9d_row9_col1, #T_e2d9d_row9_col2, #T_e2d9d_row9_col3, #T_e2d9d_row9_col4, #T_e2d9d_row9_col5, #T_e2d9d_row9_col6, #T_e2d9d_row9_col7, #T_e2d9d_row9_col8, #T_e2d9d_row9_col9, #T_e2d9d_row9_col10, #T_e2d9d_row10_col0, #T_e2d9d_row10_col1, #T_e2d9d_row10_col2, #T_e2d9d_row10_col3, #T_e2d9d_row10_col4, #T_e2d9d_row10_col5, #T_e2d9d_row10_col6, #T_e2d9d_row10_col7, #T_e2d9d_row10_col8, #T_e2d9d_row10_col9, #T_e2d9d_row10_col10, #T_e2d9d_row11_col0, #T_e2d9d_row11_col1, #T_e2d9d_row11_col2, #T_e2d9d_row11_col3, #T_e2d9d_row11_col4, #T_e2d9d_row11_col5, #T_e2d9d_row11_col6, #T_e2d9d_row11_col7, #T_e2d9d_row11_col8, #T_e2d9d_row11_col9, #T_e2d9d_row11_col10, #T_e2d9d_row12_col0, #T_e2d9d_row12_col1, #T_e2d9d_row12_col2, #T_e2d9d_row12_col3, #T_e2d9d_row12_col4, #T_e2d9d_row12_col5, #T_e2d9d_row12_col6, #T_e2d9d_row12_col7, #T_e2d9d_row12_col8, #T_e2d9d_row12_col9, #T_e2d9d_row12_col10, #T_e2d9d_row13_col0, #T_e2d9d_row13_col1, #T_e2d9d_row13_col2, #T_e2d9d_row13_col3, #T_e2d9d_row13_col4, #T_e2d9d_row13_col5, #T_e2d9d_row13_col6, #T_e2d9d_row13_col7, #T_e2d9d_row13_col8, #T_e2d9d_row13_col9, #T_e2d9d_row13_col10, #T_e2d9d_row19_col0, #T_e2d9d_row19_col1, #T_e2d9d_row19_col2, #T_e2d9d_row19_col3, #T_e2d9d_row19_col4, #T_e2d9d_row19_col5, #T_e2d9d_row19_col6, #T_e2d9d_row19_col7, #T_e2d9d_row19_col8, #T_e2d9d_row19_col9, #T_e2d9d_row19_col10, #T_e2d9d_row20_col0, #T_e2d9d_row20_col1, #T_e2d9d_row20_col2, #T_e2d9d_row20_col3, #T_e2d9d_row20_col4, #T_e2d9d_row20_col5, #T_e2d9d_row20_col6, #T_e2d9d_row20_col7, #T_e2d9d_row20_col8, #T_e2d9d_row20_col9, #T_e2d9d_row20_col10, #T_e2d9d_row21_col0, #T_e2d9d_row21_col1, #T_e2d9d_row21_col2, #T_e2d9d_row21_col3, #T_e2d9d_row21_col4, #T_e2d9d_row21_col5, #T_e2d9d_row21_col6, #T_e2d9d_row21_col7, #T_e2d9d_row21_col8, #T_e2d9d_row21_col9, #T_e2d9d_row21_col10, #T_e2d9d_row22_col0, #T_e2d9d_row22_col1, #T_e2d9d_row22_col2, #T_e2d9d_row22_col3, #T_e2d9d_row22_col4, #T_e2d9d_row22_col5, #T_e2d9d_row22_col6, #T_e2d9d_row22_col7, #T_e2d9d_row22_col8, #T_e2d9d_row22_col9, #T_e2d9d_row22_col10, #T_e2d9d_row23_col0, #T_e2d9d_row23_col1, #T_e2d9d_row23_col2, #T_e2d9d_row23_col3, #T_e2d9d_row23_col4, #T_e2d9d_row23_col5, #T_e2d9d_row23_col6, #T_e2d9d_row23_col7, #T_e2d9d_row23_col8, #T_e2d9d_row23_col9, #T_e2d9d_row23_col10 { background-color: #CCECFF; } #T_e2d9d_row4_col0, #T_e2d9d_row4_col1, #T_e2d9d_row4_col2, #T_e2d9d_row4_col3, #T_e2d9d_row4_col4, #T_e2d9d_row4_col5, #T_e2d9d_row4_col6, #T_e2d9d_row4_col7, #T_e2d9d_row4_col8, #T_e2d9d_row4_col9, #T_e2d9d_row4_col10, #T_e2d9d_row5_col0, #T_e2d9d_row5_col1, #T_e2d9d_row5_col2, #T_e2d9d_row5_col3, #T_e2d9d_row5_col4, #T_e2d9d_row5_col5, #T_e2d9d_row5_col6, #T_e2d9d_row5_col7, #T_e2d9d_row5_col8, #T_e2d9d_row5_col9, #T_e2d9d_row5_col10, #T_e2d9d_row6_col0, #T_e2d9d_row6_col1, #T_e2d9d_row6_col2, #T_e2d9d_row6_col3, #T_e2d9d_row6_col4, #T_e2d9d_row6_col5, #T_e2d9d_row6_col6, #T_e2d9d_row6_col7, #T_e2d9d_row6_col8, #T_e2d9d_row6_col9, #T_e2d9d_row6_col10, #T_e2d9d_row7_col0, #T_e2d9d_row7_col1, #T_e2d9d_row7_col2, #T_e2d9d_row7_col3, #T_e2d9d_row7_col4, #T_e2d9d_row7_col5, #T_e2d9d_row7_col6, #T_e2d9d_row7_col7, #T_e2d9d_row7_col8, #T_e2d9d_row7_col9, #T_e2d9d_row7_col10, #T_e2d9d_row8_col0, #T_e2d9d_row8_col1, #T_e2d9d_row8_col2, #T_e2d9d_row8_col3, #T_e2d9d_row8_col4, #T_e2d9d_row8_col5, #T_e2d9d_row8_col6, #T_e2d9d_row8_col7, #T_e2d9d_row8_col8, #T_e2d9d_row8_col9, #T_e2d9d_row8_col10, #T_e2d9d_row14_col0, #T_e2d9d_row14_col1, #T_e2d9d_row14_col2, #T_e2d9d_row14_col3, #T_e2d9d_row14_col4, #T_e2d9d_row14_col5, #T_e2d9d_row14_col6, #T_e2d9d_row14_col7, #T_e2d9d_row14_col8, #T_e2d9d_row14_col9, #T_e2d9d_row14_col10, #T_e2d9d_row15_col0, #T_e2d9d_row15_col1, #T_e2d9d_row15_col2, #T_e2d9d_row15_col3, #T_e2d9d_row15_col4, #T_e2d9d_row15_col5, #T_e2d9d_row15_col6, #T_e2d9d_row15_col7, #T_e2d9d_row15_col8, #T_e2d9d_row15_col9, #T_e2d9d_row15_col10, #T_e2d9d_row16_col0, #T_e2d9d_row16_col1, #T_e2d9d_row16_col2, #T_e2d9d_row16_col3, #T_e2d9d_row16_col4, #T_e2d9d_row16_col5, #T_e2d9d_row16_col6, #T_e2d9d_row16_col7, #T_e2d9d_row16_col8, #T_e2d9d_row16_col9, #T_e2d9d_row16_col10, #T_e2d9d_row17_col0, #T_e2d9d_row17_col1, #T_e2d9d_row17_col2, #T_e2d9d_row17_col3, #T_e2d9d_row17_col4, #T_e2d9d_row17_col5, #T_e2d9d_row17_col6, #T_e2d9d_row17_col7, #T_e2d9d_row17_col8, #T_e2d9d_row17_col9, #T_e2d9d_row17_col10, #T_e2d9d_row18_col0, #T_e2d9d_row18_col1, #T_e2d9d_row18_col2, #T_e2d9d_row18_col3, #T_e2d9d_row18_col4, #T_e2d9d_row18_col5, #T_e2d9d_row18_col6, #T_e2d9d_row18_col7, #T_e2d9d_row18_col8, #T_e2d9d_row18_col9, #T_e2d9d_row18_col10 { background-color: #FFFFFF; } Variable Group Count Percent Goods Bads Bad rate WoE IV Raw score Aligned score 0 worst_perimeter (-inf, 91.69) 167 0.419598 2 165 0.988024 -3.888550 2.513895 5.210485 -48 1 worst_perimeter [91.69, 102.05) 58 0.145729 5 53 0.913793 -1.836605 0.327313 2.460970 32 2 worst_perimeter [102.05, 114.65) 52 0.130653 23 29 0.557692 0.292447 0.011524 -0.391866 114 3 worst_perimeter [114.65, inf) 121 0.304020 118 3 0.024793 4.196321 3.295360 -5.622885 265 4 worst_texture (-inf, 23.35) 159 0.399497 19 140 0.880503 -1.472955 0.635759 2.334961 35 5 worst_texture [23.35, 28.24) 112 0.281407 54 58 0.517857 0.452790 0.060160 -0.717772 123 6 worst_texture [28.24, 29.23) 20 0.050251 5 15 0.750000 -0.574364 0.015058 0.910494 76 7 worst_texture [29.23, 31.17) 31 0.077889 26 5 0.161290 2.172907 0.338269 -3.444541 202 8 worst_texture [31.17, inf) 76 0.190955 44 32 0.421053 0.842702 0.142667 -1.335871 141 9 worst_smoothness (-inf, 0.10) 34 0.085427 1 33 0.970588 -2.972259 0.372255 6.443867 -83 10 worst_smoothness [0.10, 0.13) 130 0.326633 33 97 0.746154 -0.553955 0.091418 1.200976 68 11 worst_smoothness [0.13, 0.14) 63 0.158291 11 52 0.825397 -1.029100 0.137566 2.231092 38 12 worst_smoothness [0.14, 0.16) 114 0.286432 60 54 0.473684 0.629609 0.119251 -1.364995 142 13 worst_smoothness [0.16, inf) 57 0.143216 43 14 0.245614 1.646391 0.386146 -3.569382 206 14 radius_error (-inf, 0.24) 111 0.278894 6 105 0.945946 -2.337952 0.887158 3.846413 -8 15 radius_error [0.24, 0.41) 152 0.381910 35 117 0.769737 -0.682577 0.158026 1.122980 70 16 radius_error [0.41, 0.48) 32 0.080402 23 9 0.281250 1.462518 0.174633 -2.406144 172 17 radius_error [0.48, 0.56) 26 0.065327 12 14 0.538462 0.370098 0.009282 -0.608887 120 18 radius_error [0.56, inf) 77 0.193467 72 5 0.064935 3.191477 1.488781 -5.250637 254 19 worst_concavity (-inf, 0.21) 182 0.457286 4 178 0.978022 -3.271241 2.240711 2.492782 31 20 worst_concavity [0.21, 0.26) 39 0.097990 12 27 0.692308 -0.286682 0.007717 0.218460 96 21 worst_concavity [0.26, 0.29) 20 0.050251 14 6 0.300000 1.371547 0.096824 -1.045159 133 22 worst_concavity [0.29, 0.38) 53 0.133166 26 27 0.509434 0.486508 0.032925 -0.370734 113 23 worst_concavity [0.38, inf) 104 0.261307 92 12 0.115385 2.561131 1.469120 -1.951657 159 El modelo tiene una hoja de c\u00e1lculo con la scorecard pintanda con los formatos adecuados. modelo1.save_excel('scorecard_ejemplo01.xlsx')","title":"Ejemplo sencillo"},{"location":"theory/","text":"Theory Overview Binary classification is the task of classifying the elements of a set into two groups. It is a type of supervised learning , a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. So, in summary, we only have to decide to which group each element of a set will belong to, given another set of similar elements where each of them is already assigned to one of these two groups. Actually, the best thing would be to estimate the probability of belonging to each group for each element, and that is exactly what several machine learning algorithms do. However, the interpretability of these algorithms to determine what makes each element belong to a particular group is not always very clear. For example, if we use a sophisticated algorithm such as Gradient Boosting, then we may not be able to fully understand what it is doing to assign each element to a group. This is sometimes known as a black-box model behavior where there may be hundreds or even thousands of variables involved that are combined in different ways. A scorecard is a machine learning model for solving binary classification problems with full interpretability. It usually has a reduced number of variables (far from the hundreds or thousands that we mentioned before) where each of them has been previously grouped. It usually looks something like this: The two key points of a scorecard can be seen in this easy example: Each variable, regardless of whether it is numerical or categorical, has been discretized into groups. These groupings can be done automatically (using different approaches) or manually where the model developer decides in an expert way how these groupings should be. The score associated with each element, which is nothing more than the probability of belonging to one of the groups (as we will explain), is fully interpretable and traceable. Theoretical disclaimer Before moving on to the perhaps complicated theoretical part I want to point out that the best way to learn how to use this library is by using the example notebooks. For a deep understanding of the logic that Pyken follows in its scorecard generation process it is recommended to read the following sections, but I emphasize that it is not necessary to use it. Details of the many features of the autoscorecard class can be found in the last chapter. Data processing La idea ahora es contar como trata Pyken los distintos tipos de dato y sus valores missings...","title":"Theory"},{"location":"theory/#theory","text":"","title":"Theory"},{"location":"theory/#overview","text":"Binary classification is the task of classifying the elements of a set into two groups. It is a type of supervised learning , a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. So, in summary, we only have to decide to which group each element of a set will belong to, given another set of similar elements where each of them is already assigned to one of these two groups. Actually, the best thing would be to estimate the probability of belonging to each group for each element, and that is exactly what several machine learning algorithms do. However, the interpretability of these algorithms to determine what makes each element belong to a particular group is not always very clear. For example, if we use a sophisticated algorithm such as Gradient Boosting, then we may not be able to fully understand what it is doing to assign each element to a group. This is sometimes known as a black-box model behavior where there may be hundreds or even thousands of variables involved that are combined in different ways. A scorecard is a machine learning model for solving binary classification problems with full interpretability. It usually has a reduced number of variables (far from the hundreds or thousands that we mentioned before) where each of them has been previously grouped. It usually looks something like this: The two key points of a scorecard can be seen in this easy example: Each variable, regardless of whether it is numerical or categorical, has been discretized into groups. These groupings can be done automatically (using different approaches) or manually where the model developer decides in an expert way how these groupings should be. The score associated with each element, which is nothing more than the probability of belonging to one of the groups (as we will explain), is fully interpretable and traceable.","title":"Overview"},{"location":"theory/#theoretical-disclaimer","text":"Before moving on to the perhaps complicated theoretical part I want to point out that the best way to learn how to use this library is by using the example notebooks. For a deep understanding of the logic that Pyken follows in its scorecard generation process it is recommended to read the following sections, but I emphasize that it is not necessary to use it. Details of the many features of the autoscorecard class can be found in the last chapter.","title":"Theoretical disclaimer"},{"location":"theory/#data-processing","text":"La idea ahora es contar como trata Pyken los distintos tipos de dato y sus valores missings...","title":"Data processing"}]}